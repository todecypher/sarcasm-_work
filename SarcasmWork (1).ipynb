{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport re\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# Any results you write to the current directory are saved as output.\nfrom sklearn.ensemble import  RandomForestClassifier as RFC\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\nfrom keras.layers.embeddings import Embedding\nimport nltk\nimport string\nimport numpy as np\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import f1_score,accuracy_score ,roc_auc_score\nfrom nltk.stem.snowball import SnowballStemmer\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.linear_model import LogisticRegression as LR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/sarcasm/tweets.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df= df.dropna()\ndf = df[df.tweet.apply(lambda x: x !=\"\")]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    \n    ## Remove puncuation\n    text = text.translate(string.punctuation)\n    \n    ## Convert words to lower case and split them\n    text = text.lower().split()\n    \n    \n    ## Remove stop words\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops and len(w) >= 3]\n    \n    text = \" \".join(text)\n    ## Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    ## Stemming\n    text = text.split()\n    stemmer = SnowballStemmer('english')\n    stemmed_words = [stemmer.stem(word) for word in text]\n    text = \" \".join(stemmed_words)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"tweet\"] = df[\"tweet\"].map(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Create sequence\nvocabulary_size = 20000\ntokenizer = Tokenizer(num_words= vocabulary_size)\ntokenizer.fit_on_texts(df[\"tweet\"])\nsequences = tokenizer.texts_to_sequences(df[\"tweet\"])\ndata = pad_sequences(sequences, maxlen=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Extract word embeddings from the Glove**"},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = dict()\nf = open('/kaggle/input/glove/glove.6B.50d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create a weight matrix**"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((vocabulary_size, 50))\nfor word, index in tokenizer.word_index.items():\n    if index > vocabulary_size - 1:\n        break\n    else:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[index] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras import backend as K\nK.tensorflow_backend._get_available_gpus()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nconfig = tf.ConfigProto( device_count = {'GPU': 1 } ) \nsess = tf.Session(config=config) \nkeras.backend.set_session(sess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = df[\"Sarcasm\"]\nmodel_glove = Sequential()\nmodel_glove.add(Embedding(vocabulary_size, 50, input_length=50, weights=[embedding_matrix], trainable=True))\nmodel_glove.add(Dropout(0.2))\nmodel_glove.add(Conv1D(64, 5, activation='relu'))\nmodel_glove.add(MaxPooling1D(pool_size=4))\nmodel_glove.add(LSTM(100))\nmodel_glove.add(Dense(1, activation='sigmoid'))\nmodel_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'] )\n## Fit train data\nmodel_glove.fit(data, np.array(labels), validation_split=0.2, epochs = 10 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NN = model_glove","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = tts(data, labels, test_size=0.2,  random_state=13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = xgb.XGBClassifier(n_jobs=-1 , n_estimatore =200)\nmodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RFC(n_jobs= -1)\nrfc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LR()\nlr.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [NN ,rfc, lr,model] \nf1score =[]\nauc_score = []\naccuracy = []\nfor i in models :\n    pred = np.round(i.predict(X_test)).astype(int)\n    f1score.append(f1_score(pred,y_test))\n    auc_score.append(roc_auc_score(pred,y_test))\n    accuracy.append(accuracy_score(pred,y_test))\n    \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame()\nresults[\"MODEL\"] =[\"Neaural Net with LSTM\" ,\"Random Forest Classifier\" , \"Logistic Regression\" ,\"XGBOOST Classifier\"]\nresults[\"F1_Score\"] =f1score\nresults[\"Auc_Score\"] =auc_score\nresults[\"Accuracy %\"]=accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}